{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33012a14",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dask/dask/main/docs/source/images/dask_horizontal_no_pad.svg\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\" />\n",
    "\n",
    "# Parallel and Distributed Machine Learning\n",
    "\n",
    "The material in this notebook was based on the open-source content from [Dask's tutorial repository](https://github.com/dask/dask-tutorial) and the [Machine learning notebook](https://github.com/coiled/data-science-at-scale/blob/master/3-machine-learning.ipynb) from data science at scale from coiled\n",
    "\n",
    "So far we have seen how Dask makes data analysis scalable with parallelization via Dask DataFrames. Let's now see how [Dask-ML](https://ml.dask.org/) allows us to do machine learning in a parallel and distributed manner. Note, machine learning is really just a special case of data analysis (one that automates analytical model building), so the ðŸ’ª Dask gains ðŸ’ª we've seen will apply here as well!\n",
    "\n",
    "(If you'd like a refresher on the difference between parallel and distributed computing, [here's a good discussion on StackExchange](https://cs.stackexchange.com/questions/1580/distributed-vs-parallel-computing).)\n",
    "\n",
    "\n",
    "## Types of scaling problems in machine learning\n",
    "\n",
    "There are two main types of scaling challenges you can run into in your machine learning workflow: scaling the **size of your data** and scaling the **size of your model**. That is:\n",
    "\n",
    "1. **CPU-bound problems**: Data fits in RAM, but training takes too long. Many hyperparameter combinations, a large ensemble of many models, etc.\n",
    "2. **Memory-bound problems**: Data is larger than RAM, and sampling isn't an option.\n",
    "\n",
    "Here's a handy diagram for visualizing these problems:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/coiled/data-science-at-scale/master/images/dimensions_of_scale.svg\"\n",
    "     width=\"60%\"\n",
    "     alt=\"scaling problems\\\" />\n",
    "\n",
    "\n",
    "In the bottom-left quadrant, your datasets are not too large (they fit comfortably in RAM) and your model is not too large either. When these conditions are met, you are much better off using something like scikit-learn, XGBoost, and similar libraries. You don't need to leverage multiple machines in a distributed manner with a library like Dask-ML. However, if you are in any of the other quadrants, distributed machine learning is the way to go.\n",
    "\n",
    "Summarizing: \n",
    "\n",
    "* For in-memory problems, just use scikit-learn (or your favorite ML library).\n",
    "* For large models, use `dask_ml.joblib` and your favorite scikit-learn estimator.\n",
    "* For large datasets, use `dask_ml` estimators.\n",
    "\n",
    "## Scikit-learn in five minutes\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/coiled/data-science-at-scale/master/images/scikit_learn_logo_small.svg\" \n",
    "     width=\"30%\"\n",
    "     alt=\"sklearn logo\\\" />\n",
    "\n",
    "\n",
    "In this section, we'll quickly run through a typical scikit-learn workflow:\n",
    "\n",
    "* Load some data (in this case, we'll generate it)\n",
    "* Import the scikit-learn module for our chosen ML algorithm\n",
    "* Create an estimator for that algorithm and fit it with our data\n",
    "* Inspect the learned attributes\n",
    "* Check the accuracy of our model\n",
    "\n",
    "Scikit-learn has a nice, consistent API:\n",
    "\n",
    "* You instantiate an `Estimator` (e.g. `LinearRegression`, `RandomForestClassifier`, etc.). All of the models *hyperparameters* (user-specified parameters, not the ones learned by the estimator) are passed to the estimator when it's created.\n",
    "* You call `estimator.fit(X, y)` to train the estimator.\n",
    "* Use `estimator` to inspect attributes, make predictions, etc. \n",
    "\n",
    "Here `X` is an array of *feature variables* (what you're using to predict) and `y` is an array of *target variables* (what we're trying to predict)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7379d",
   "metadata": {},
   "source": [
    "### Generate some random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate data\n",
    "X, y = make_classification(n_samples=10000, n_features=4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbcf18d",
   "metadata": {},
   "source": [
    "**Refreshing some ML concepts**\n",
    "\n",
    "- `X` is the samples matrix (or design matrix). The size of `X` is typically (`n_samples`, `n_features`), which means that samples are represented as rows and features are represented as columns.\n",
    "- A \"feature\" (also called an \"attribute\") is a measurable property of the phenomenon we're trying to analyze. A feature for a dataset of employees might be their hire date, for example.\n",
    "- `y` are the target values, which are real numbers for regression tasks, or integers for classification (or any other discrete set of values). For unsupervized learning tasks, `y` does not need to be specified. `y` is usually 1d array where the `i`th entry corresponds to the target of the `i`th sample (row) of `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85824219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at X\n",
    "X[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at y\n",
    "y[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d61bf2",
   "metadata": {},
   "source": [
    "### Fitting and SVC\n",
    "\n",
    "For this example, we will fit a [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d638ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "estimator = SVC(random_state=0)\n",
    "estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4068406a",
   "metadata": {},
   "source": [
    "We can inspect the learned features by taking a look a the `support_vectors_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b891e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.support_vectors_[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a175852d",
   "metadata": {},
   "source": [
    "And we check the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efac1bd",
   "metadata": {},
   "source": [
    "There are [3 different approaches](https://scikit-learn.org/0.15/modules/model_evaluation.html) to evaluate the quality of predictions of a model. One of them is the **estimator score method**. Estimators have a score method providing a default evaluation criterion for the problem they are designed to solve, which is discussed in each estimator's documentation.\n",
    "\n",
    "### Hyperparameter Optimization\n",
    "\n",
    "There are a few ways to learn the best *hyper*parameters while training. One is `GridSearchCV`.\n",
    "As the name implies, this does a brute-force search over a grid of hyperparameter combinations. scikit-learn provides tools to automatically find the best parameter combinations via cross-validation (which is the \"CV\" in `GridSearchCV`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4659297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c889f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator = SVC(gamma='auto', random_state=0, probability=True)\n",
    "param_grid = {\n",
    "    'C': [0.001, 10.0],\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "}\n",
    "\n",
    "# Brute-force search over a grid of hyperparameter combinations\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad134157",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228deb1b",
   "metadata": {},
   "source": [
    "## Compute Bound: Single-machine parallelism with Joblib\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/coiled/data-science-at-scale/master/images/joblib_logo.svg\" \n",
    "     alt=\"Joblib logo\" \n",
    "     width=\"50%\"/>\n",
    "\n",
    "In this section we'll see how [Joblib](https://joblib.readthedocs.io/en/latest/) (\"*a set of tools to provide lightweight pipelining in Python*\") gives us parallelism on our laptop. Here's what our grid search graph would look like if we set up six training \"jobs\" in parallel:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/coiled/data-science-at-scale/master/images/unmerged_grid_search_graph.svg\" \n",
    "     alt=\"grid search graph\" \n",
    "     width=\"100%\"/>\n",
    "\n",
    "With Joblib, we can say that scikit-learn has *single-machine* parallelism.\n",
    "Any scikit-learn estimator that can operate in parallel exposes an `n_jobs` keyword, which tells you how many tasks to run in parallel. Specifying `n_jobs=-1` jobs means running the maximum possible number of tasks in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bead16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2, n_jobs=-1)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c73f87",
   "metadata": {},
   "source": [
    "Notice that the computation above it is faster than before. If you are running this computation on binder, you might not see a speed-up and the reason for that is that binder instances tend to have only one core with no threads so you can't see any parallelism. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe6255f",
   "metadata": {},
   "source": [
    "## Compute Bound: Multi-machine parallelism with Dask\n",
    "\n",
    "\n",
    "In this section we'll see how Dask (plus Joblib and scikit-learn) gives us multi-machine parallelism. Here's what our grid search graph would look like if we allowed Dask to schedule our training \"jobs\" over multiple machines in our cluster:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/coiled/data-science-at-scale/master/images/merged_grid_search_graph.svg\" \n",
    "     alt=\"merged grid search graph\" \n",
    "     width=\"100%\"/>\n",
    "     \n",
    "We can say that Dask can talk to scikit-learn (via Joblib) so that our *cluster* is used to train a model. \n",
    "\n",
    "If we run this on a laptop, it will take quite some time, but the CPU usage will be satisfyingly near 100% for the duration. To run faster, we would need a distributed cluster. For details on how to create a LocalCluster you can check the Dask documentation on [Single Machine: dask.distributed](https://docs.dask.org/en/latest/setup/single-distributed.html). \n",
    "\n",
    "Let's instantiate a Client with `n_workers=4`, which will give us a `LocalCluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf776a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.distributed\n",
    "\n",
    "client = dask.distributed.Client(n_workers=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c77aa",
   "metadata": {},
   "source": [
    "**Note:** Click on Cluster Info, to see more details about the cluster. You can see the configuration of the cluster and some other specs. \n",
    "\n",
    "We can expand our problem by specifying more hyperparameters before training, and see how using `dask` as backend can help us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f1c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.1, 1.0, 2.5, 5, 10.0],\n",
    "    'kernel': ['rbf', 'poly', 'linear'],\n",
    "    'shrinking': [True, False],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2854c927",
   "metadata": {},
   "source": [
    "### Dask parallel backend\n",
    "\n",
    "We can fit our estimator with multi-machine parallelism by quickly *switching to a Dask parallel backend* when using joblib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb681a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8228e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with joblib.parallel_backend(\"dask\", scatter=[X, y]):\n",
    "    grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9194cf",
   "metadata": {},
   "source": [
    "**What did just happen?**\n",
    "\n",
    "Dask-ML developers worked with the scikit-learn and Joblib developers to implement a Dask parallel backend. So internally, scikit-learn now talks to Joblib, and Joblib talks to Dask, and Dask is what handles scheduling all of those tasks on multiple machines.\n",
    "\n",
    "The best parameters and best score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea35f8",
   "metadata": {},
   "source": [
    "## Memory Bound: Single/Multi machine parallelism with Dask-ML\n",
    "\n",
    "We have seen how to work with larger models, but sometimes you'll want to train on a larger than memory dataset. `dask-ml` has implemented estimators that work well on Dask `Arrays` and `DataFrames` that may be larger than your machine's RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ed30f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask.delayed\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebd444f",
   "metadata": {},
   "source": [
    "We'll make a small (random) dataset locally using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f703b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_centers = 12\n",
    "n_features = 20\n",
    "\n",
    "X_small, y_small = make_blobs(n_samples=1000, centers=n_centers, n_features=n_features, random_state=0)\n",
    "\n",
    "centers = np.zeros((n_centers, n_features))\n",
    "\n",
    "for i in range(n_centers):\n",
    "    centers[i] = X_small[y_small == i].mean(0)\n",
    "    \n",
    "centers[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f3881",
   "metadata": {},
   "source": [
    "**Note**: The small dataset will be the template for our large random dataset.\n",
    "We'll use `dask.delayed` to adapt `sklearn.datasets.make_blobs`, so that the actual dataset is being generated on our workers. \n",
    "\n",
    "If you are not in binder and you machine has 16GB of RAM you can make `n_samples_per_block=200_000` and the computations takes around 10 min. If you are in binder the resources are limited and the problem below is big enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d91e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_block = 60_000 #on binder replace this for 15_000\n",
    "n_blocks = 500\n",
    "\n",
    "delayeds = [dask.delayed(make_blobs)(n_samples=n_samples_per_block,\n",
    "                                     centers=centers,\n",
    "                                     n_features=n_features,\n",
    "                                     random_state=i)[0]\n",
    "            for i in range(n_blocks)]\n",
    "arrays = [da.from_delayed(obj, shape=(n_samples_per_block, n_features), dtype=X.dtype)\n",
    "          for obj in delayeds]\n",
    "X = da.concatenate(arrays)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc609b5f",
   "metadata": {},
   "source": [
    "### KMeans from Dask-ml\n",
    "\n",
    "The algorithms implemented in Dask-ML are scalable. They handle larger-than-memory datasets just fine.\n",
    "\n",
    "They follow the scikit-learn API, so if you're familiar with scikit-learn, you'll feel at home with Dask-ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095e644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a4b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KMeans(init_max_iter=3, oversampling_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a1e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30535766",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_[:10].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a83d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103498a",
   "metadata": {},
   "source": [
    "##  Multi-machine parallelism in the cloud with Coiled\n",
    "\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/coiled/data-science-at-scale/master/images/Coiled-Logo_Horizontal_RGB_Black.png\"\n",
    "     alt=\"Coiled logo\" \n",
    "     width=25%/>\n",
    "<br>\n",
    "\n",
    "In this section we'll see how Coiled allows us to solve machine learning problems with multi-machine parallelism in the cloud.\n",
    "\n",
    "Coiled, [among other things](https://coiled.io/product/), provides hosted and scalable Dask clusters. The biggest barriers to entry for doing machine learning at scale are \"Do you have access to a cluster?\" and \"Do you know how to manage it?\" Coiled solves both of those problems. \n",
    "\n",
    "We'll spin up a Coiled cluster (with 10 workers in this case), then instantiate a Dask Client to use with that cluster.\n",
    "\n",
    "If you are running on your local machine and not in binder, and you want to give Coiled a try, you can signup [here](https://cloud.coiled.io/login?redirect_uri=/) and you will get some free credits. If you installed the environment by following the steps on the repository's [README](https://github.com/coiled/dask-mini-tutorial/blob/main/README.md) you will have `coiled` installed. You will just need to login, by following the steps on the [setup page](https://docs.coiled.io/user_guide/getting_started.html), and you will be ready to go. \n",
    "\n",
    "To learn more about how to set up an environment you can visit Coiled documentation on [Creating software environments](https://docs.coiled.io/user_guide/software_environment_creation.html). But for now you can use the envioronment we set up for this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4268dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spin up a Coiled cluster, instantiate a Client\n",
    "cluster = coiled.Cluster(n_workers=10, software=\"ncclementi/dask-mini-tutorial\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ddd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3980aa",
   "metadata": {},
   "source": [
    "### Memory bound: Dask-ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efccea68",
   "metadata": {},
   "source": [
    "We can use Dask-ML estimators on the cloud to work with larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc52c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_centers = 12\n",
    "n_features = 20\n",
    "\n",
    "X_small, y_small = make_blobs(n_samples=1000, centers=n_centers, n_features=n_features, random_state=0)\n",
    "\n",
    "centers = np.zeros((n_centers, n_features))\n",
    "\n",
    "for i in range(n_centers):\n",
    "    centers[i] = X_small[y_small == i].mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c77b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_block = 200_000\n",
    "n_blocks = 500\n",
    "\n",
    "delayeds = [dask.delayed(make_blobs)(n_samples=n_samples_per_block,\n",
    "                                     centers=centers,\n",
    "                                     n_features=n_features,\n",
    "                                     random_state=i)[0]\n",
    "            for i in range(n_blocks)]\n",
    "arrays = [da.from_delayed(obj, shape=(n_samples_per_block, n_features), dtype=X.dtype)\n",
    "          for obj in delayeds]\n",
    "X = da.concatenate(arrays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d891a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fef03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a186141",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KMeans(init_max_iter=3, oversampling_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56241fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3682c1",
   "metadata": {},
   "source": [
    "Computing the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b9980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_[:10].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9988ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f839e",
   "metadata": {},
   "source": [
    "## Extra resources:\n",
    "\n",
    "- [Dask-ML documentation](https://ml.dask.org/)\n",
    "- [Getting started with Coiled](https://docs.coiled.io/user_guide/getting_started.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
